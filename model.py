#!/usr/bin/env python
import cv2
import numpy as np
import matplotlib.pyplot as plt
import torch
import math
from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
import warnings
from torch import optim
from torch.optim.lr_scheduler import StepLR
from sklearn.mixture import GaussianMixture
from skimage.transform import iradon
from copy import deepcopy
from tqdm import tqdm
import pandas as pd
import os

from config import *
from anatomy import *
from renderer import *
from siren import *


def get_sinogram(config, sdf, intensities, all_thetas = None, offset = 0.0):
    renderer = Renderer(config, sdf,intensities,offset=offset)
    if all_thetas is None:
        all_thetas = np.linspace(0,config.THETA_MAX, config.TOTAL_CLICKS)
    sinogram = renderer.forward(all_thetas).detach().cpu().numpy()
    
    return sinogram


def fetch_fbp_movie_exp1(config, body, gantry_offset=0.0):

    ''' this function generates sinogram (projections at number_of_theta) from differentiable rendering and sdf'''
    
    intensities = Intensities(config, learnable = False)
    sdf = SDFGt(config, body)
    incr = 180 # what is incr here?
    with torch.no_grad():
        
        # all_thetas obtains a list of theta for each gantry view (which is t)
        all_thetas = np.linspace(-config.THETA_MAX/2-incr/2, config.THETA_MAX/2+incr/2, config.TOTAL_CLICKS + incr*int(config.GANTRY_VIEWS_PER_ROTATION/360))  # gantry_views_per_rotation/360 calculates how many views can be added for one additional degree

        gtrenderer = Renderer(config,sdf,intensities,offset=gantry_offset)
        sinogt = gtrenderer(all_thetas).detach().cpu().numpy()  # forward function in renderer, shape is [x_dim, number_of_thetas], Image_resolution =  x_dim, each projection is 1D, num_of_thetas = number_of_projections

        print('sinogram generated by differentiatble rendering from snapshot in renderer: ',sinogt.shape) 

    sinogram = sinogt.reshape(config.IMAGE_RESOLUTION, config.TOTAL_CLICKS + incr*int(config.GANTRY_VIEWS_PER_ROTATION/360)) # 720views(total_clicks) + 360views(incr) = 1080, gantry_views_per_rotation/360 calculates how many views/clicks can be added for one additional degree
  
    reconstruction_fbp = np.zeros((config.IMAGE_RESOLUTION,config.IMAGE_RESOLUTION,config.TOTAL_CLICKS+ 0*incr*int(config.GANTRY_VIEWS_PER_ROTATION/360)))
    count=0
    
    # filtered back projection algorithm:
    # skimage.transform.iradon - inverse radon transform, reconstruct an image from the radon transform, using the filtered back-projection algorithm
    # iradon(radon_image, theta=None, output_size = None, filter_name = 'ramp', interpolation = 'linear), radom_image = sinogram, each colume of the image corresponds to a projection along the different angle, theta = reconstruction angles

    print('increment has how many projections/clicks: ', incr*int(config.GANTRY_VIEWS_PER_ROTATION/360))
    for i in tqdm(range(0, config.TOTAL_CLICKS + 0*incr*int(config.GANTRY_VIEWS_PER_ROTATION/360))):
        # scipy.ndimage.rotate(input,angle), here angle = -gantry_offset

        reconstruction_fbp[...,count] = rotate(iradon(sinogram[...,i:i+incr*int(config.GANTRY_VIEWS_PER_ROTATION/360)],  # so each time when we do filtered-backprojection we use incr*int(config.GANTRY_VIEWS_PER_ROTATION/360) = 360 views? Yes.
                                               theta = all_thetas[i:i+incr*int(config.GANTRY_VIEWS_PER_ROTATION/360)] + gantry_offset ,circle=True).T, 
                                               0,reshape = False)
                                               #-gantry_offset, reshape=False)
        count+=1
    print('sinogram has shape: ', sinogram.shape, ' rereconstructino fbp: ', np.max(reconstruction_fbp[:,:,300]), reconstruction_fbp.shape, ' after scale it becomes: ', np.max(reconstruction_fbp[:,:,300]) * 132)
    # reconstrucion_fbp.shape  = [Image_resolution, Image_resolution, number_of_gantry_total_click]

    return sinogram, 132*reconstruction_fbp  # why 132? it's a scaling factor because reconstruction fbp will generate a very very small value. This 132 was empirically set, but theorectially it's deterministic and dependent on the CT scan not on the image.


# modified by Zhennong Chen
def get_n_objects(img, num_components=2, threshold = None, save_GMM_label_path = None,  save_label = True):

    '''this function uses Gaussian mixture model to fit components for an image. It returns an image with each pixel labels, num_componenets = 1 for binary circle'''
    
    assert isinstance(img, np.ndarray) and len(img.shape) == 3, 'img must be a 3D numpy array'
    assert isinstance(num_components, int), 'num_components must be a integer'
    assert isinstance(threshold, float)

    num_components+=3 # have a +3 class buffer
    proceed = True
    count = 0

    while proceed and count<3: # only try three times at most
        mask = np.random.randint(0,img.shape[2],int(0.02*img.shape[2]))
        X = img[...,mask].reshape(-1,1) 
        
        # A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distribution with unknown parameters. 
        # One can thick of mixture models as generalizing k-means clustering.
        gm = GaussianMixture(n_components=num_components, random_state=0).fit(X)
    
        # gm.means_ will return means for 5 clusters.
        # labels_to_use are clusters with means larger than 0.15.

        print('GAUSSIAN MIXTURE MODEL MEANS: ',gm.means_)
        labels_to_use = np.where(gm.means_[:,0]>threshold)[0]  
        GMM_means = gm.means_.copy()

        if labels_to_use.shape[0] >= num_components-3:
            print('Found labels : {} needed {}. Tried {} times'.format(labels_to_use,num_components-3,count+1))
            proceed = False
        else:
            print('Segmentation failed, found labels : {} needed {}. Tried {} times'.format(labels_to_use,num_components-3,count+1))
            count+=1
                 
    labels = np.zeros((img.shape))

    # enumerate each gantry click recon
    for i in range(img.shape[2]):
        label_image = np.zeros((img.shape[0],img.shape[1],labels_to_use.shape[0]))

        count=1
        sizes = []

        # enumerate each label
        for idx, k in enumerate(labels_to_use):
            
            # obtain the label for each pixel in the image. 
            label_predicts = gm.predict(img[...,i].reshape(-1,1)) 
            # turn the label value of each pixel into the boolean variable by assert pixel_label == current_label
            lbi = (gm.predict(img[...,i].reshape(-1,1)).reshape(img.shape[0],img.shape[1]) == k)

            label_image[...,idx] = lbi*count 
            sizes.append(np.sum(lbi))
            count+=1
        
        sizes2 = sizes    
        sizes2.sort(reverse=True)  ## zhennong's edit: reverse = True

        if i == 0:
            print('size and size2', sizes, sizes2, num_components - 3, sizes2[num_components-3:])
        
        if sizes2[num_components-3:] != []: # remove labels with largest size
            labels_to_remove = []
            for jj in sizes2[num_components-3:]:
                labels_to_remove.append(sizes.index(jj))
            label_image = np.delete(label_image,labels_to_remove,axis=2)

        label_image = np.sum(label_image, axis=2)
        if i == 0:
            print('label image : ',label_image.shape, np.unique(label_image))

        labels[...,i] = label_image

    if save_label == True:
        labels = labels.reshape(img.shape)
        np.save(save_GMM_label_path,labels)

        # also save the info of GMM derived mean and size of each class:
        txt_path = os.path.join(os.path.dirname(save_GMM_label_path),'GMM_infos.txt')
        if os.path.isfile(txt_path) == 1:
            os.remove(txt_path)
        t_file = open(txt_path,"w+")
        t_file.write("GMM obtained mean values as: ")
        for ii in range(0,GMM_means.shape[0]):
            t_file.write(" %.4f " % (GMM_means[ii,0]))
        t_file.write("\nGMM decided to use label by setting threshold > %.2f: " % threshold)
        for ii in range(0,labels_to_use.shape[0]):
            t_file.write(" %d " % labels_to_use[ii])
        t_file.write("\nThe sizes of each label are: ")
        for ii in range(0,len(sizes)):
            t_file.write(" %d " % sizes[ii])
        t_file.close()

    return labels.reshape(img.shape)


def get_n_objects_for_movie(fbp_movie, num_components=2,  GMM_threshold = None, save_GMM_label_path = None): # fbp_movie is the recon matrix with each channel as one gantry click
    '''this function returns a movie_object (Size: (x_dim, y_dim, gantry_clicks, num_SDF)) which assigns SDF values to each pixel in the fbp recon.'''
    ''' init is the median intensity for pixels belonged to that componenet.'''
    print("Computing Segmentations...")
    assert isinstance(GMM_threshold, float)
    movie = get_n_objects(fbp_movie.copy(), num_components=num_components, threshold = GMM_threshold, save_GMM_label_path = save_GMM_label_path)
  
    movie_objects = np.zeros((movie.shape[0],movie.shape[1],movie.shape[2],num_components))
    labels = np.arange(0,np.max(movie[...,0])).astype(np.int) # labels are all the labels except the last one, e.g. image's labels = [0,1], the variable labels = [0]


    for i in range(movie.shape[2]):
        for l in labels:
            movie_objects[...,i,l] = (movie[...,i] ==l+1)
    
    print("Computing SDFs...")
    init = np.zeros((1,num_components))
    for j in tqdm(range(num_components)):
        for i in range(movie.shape[2]):
            occupancy = np.round(denoise_tv_chambolle(movie_objects[...,i,j][...,np.newaxis])) # add one more dimension for occupancy, e.g (x_dim, y_dim, 720, num_components) -> (x_dim, y_dim, 720, num_compoenents, 1)
            movie_objects[...,i,j] = denoise_tv_chambolle(occ_to_sdf(occupancy), weight=2)[...,0] #the greater weight, the more denoising
        img = fbp_movie[...,j]
        test = np.where(movie_objects[...,0,j]>0)#[...,0]  # all the pixels in the first gantry image with SDF > 0
        init[0,j] = np.median(img[test[0], test[1]]) # test[0] is x coordiante and test[1] is y_coordinate

    return movie_objects, init


def get_pretraining_sdfs(config, sdf=None, GMM_threshold = None, save_GMM_label_path = None):
    if sdf is None:
        pretraining_sdfs = np.zeros((config.IMAGE_RESOLUTION,config.IMAGE_RESOLUTION,config.TOTAL_CLICKS,config.NUM_SDFS))
        for i in range(config.NUM_SDFS):
            cfg = Config(np.array([[np.random.rand()]]), config.TYPE, config.NUM_HEART_BEATS, 1)
            if i ==0:
                organ = Organ(cfg, [0.6,0.6], 0.1, 0.1, 'simple_sin', 'simple_sin2')
            else:
                organ = Organ(cfg, [0.3,0.3], 0.1, 0.1, 'simple_sin', 'simple_sin2')

            body = Body(cfg,[organ])
            sdf = SDFGt(cfg, body)
            all_thetas = np.linspace(0., config.THETA_MAX, config.TOTAL_CLICKS)
            for j in range(config.TOTAL_CLICKS):
                pretraining_sdfs[...,j,i] = denoise_tv_chambolle(sdf(all_thetas[j])[...,0].detach().cpu().numpy())
    
        init = config.INTENSITIES
    elif isinstance(sdf, np.ndarray):   # this is used before SIREN training, basically you get the pretraining_sdf from the reconstruction_fbp
        assert isinstance(GMM_threshold, float)
        pretraining_sdfs, init = get_n_objects_for_movie(sdf, num_components=config.NUM_SDFS, GMM_threshold = GMM_threshold, save_GMM_label_path = save_GMM_label_path)
        
    else:  # this is used before the SIREN refinement, bascially you get the pretrianing_sdf from trained SIREN(variable = sdf)
        pretraining_sdfs = np.zeros((config.IMAGE_RESOLUTION,config.IMAGE_RESOLUTION,config.TOTAL_CLICKS,config.NUM_SDFS))
        for i in range(config.NUM_SDFS):
            all_thetas = np.linspace(0., config.THETA_MAX, config.TOTAL_CLICKS)
            for j in range(config.TOTAL_CLICKS):
                pretraining_sdfs[...,j,i] = occ_to_sdf(np.round(denoise_tv_chambolle(sdf_to_occ(sdf(all_thetas[j]))[...,i].detach().cpu().numpy(), weight=2))[...,np.newaxis])[...,0]
          
        init = None
    return pretraining_sdfs, init


class FourierFeatures(nn.Module):
    '''
    Fit a function (multilayer perceptron, with convolution layers with kernel size = 1 equaviently as a densely connected layer) to an image
    By a naive approach and then introduce the gaussian fourier feature mapping.

    Refer: https://colab.research.google.com/github/ndahlquist/pytorch-fourier-feature-networks/blob/master/demo.ipynb#scrollTo=QDs4Im9WTQoy
    
    Check my own copy: https://colab.research.google.com/drive/1wuxDP76ZqXr6cSA8WKJMeFsxWnNkfMCw#scrollTo=zfq1-Me3akNs

    This link has been downloaded as tutorial_implicit_neural_represet_fourier_feature.ipynb. Run use the link above.
    '''

    # What is Fourier feature? :map v to y(v) = [aj*cos(2pi*bj*v), aj*sin(2pi*bj*v)], j = mapping size, simple strategy is to set aj = 1 and randomly sample bj from an isotropic distribution.
    
    # why do we need Fourier features?  Notice here we have FF for output SDF value, but conventionally when we say FF -> we map input coordinate [x,y] to FF value. Don't mix two ideas.
    # Let's assume image has 128x128, there are 1000 time frames (t)
    # without fourier features of output SDF, the output dimension of NN needs to be 1000 (one point has 1000 SDF values for 1000 time frames)
    # with fourier features, the NN doesn't care about the number of time frames. Instead, it output A, B and C (see equation 1 in the manuscript) independent of t. the number of A,B,C only depends on the mapping_size (the number of frequencies we picked)
    # In this way, we dramatically decrease the dimension of NN output from 1000 to mapping_size for each point.

    def __init__(self, input_channels, output_channels, mapping_size = 128, scale=1.5, testing=False): # in our case input_channel = 2, output_channels = num_SDF
        super(FourierFeatures, self).__init__()
        
        assert isinstance(input_channels, int), 'input_channels must be an integer'
        assert isinstance(output_channels, int), 'output_channels must be an integer'
        assert isinstance(mapping_size, int), 'maping_size must be an integer'
        assert isinstance(scale, float), 'scale must be an float'
        assert isinstance(testing, bool), 'testing should be a bool'
        
        self.mapping_size = mapping_size
        self.output_channels = output_channels
        self.testing = testing
        
        if self.testing:
            self.B = torch.ones((1, self.mapping_size, self.output_channels))
        else:
            self.B = torch.randn((1, self.mapping_size, self.output_channels))*scale # self.B is the omega (frequency) in the equation in the manuscript
    
        self.B = self.B.cuda()

        # here SIREN's output is not num_SDF but already take the foureir mapping into account (read the manuscript, these are A - for sin ,B - for cos and C - for DC values)
        self.net = Siren(input_channels,128,3,(2*self.mapping_size+1)*self.output_channels) # 128 = hidden features, 3 = num_hidden_layers, (2*self.mapping_size + 1 ) = num_foureir_features (plus one DC value), this value * out_channels = out_features of SIREN
        
    def forward(self, x, t):
        
        assert isinstance(x, torch.Tensor) and len(x.shape) == 2, 'x must be a 2D tensor'
        assert isinstance(t, torch.Tensor) or isinstance(t, float) and t>=-1 and t <=1, 't must be a float between -1 and 1'

        if self.testing:
            fourier_coeffs = torch.ones((x.shape[0],self.mapping_size*2+1, self.output_channels)).type_as(x)
        else:
            fourier_coeffs = self.net(x).view(-1, self.mapping_size*2+1, self.output_channels)
            
        fourier_coeffs_dc = fourier_coeffs[:,-1:,:] # last element in the SIREN output.
        fourier_coeffs_ac = fourier_coeffs[:,:-1,:]
        
        assert fourier_coeffs_dc.shape == (x.shape[0], 1, self.output_channels), 'Inavild size for fourier_coeffs_dc : {}'.format(fourier_coeffs_dc.shape)
        assert fourier_coeffs_ac.shape == (x.shape[0], self.mapping_size*2, self.output_channels),  'Inavild size for fourier_coeffs_ac : {}'.format(fourier_coeffs_ac.shape)

        t = (2*np.pi*t*self.B).repeat(x.shape[0],1,1)
        
        tsins = torch.cat([torch.sin(t), torch.cos(t)], dim=1).type_as(x)

        assert tsins.shape == (x.shape[0],2*self.mapping_size,self.output_channels)
        series = torch.mul(fourier_coeffs_ac, tsins)
        assert series.shape ==  (x.shape[0],2*self.mapping_size,self.output_channels)
        val_t = torch.mean(series, dim=1, keepdim=True)   # Questions!!!:   In manuscript it's the sum but here it's the mean.
        assert val_t.shape == (x.shape[0],1,self.output_channels)
        val_t = val_t + fourier_coeffs_dc
        assert val_t.shape == (x.shape[0],1,self.output_channels)
        
        return val_t.squeeze(1)
    

class SDFNCT(SDF):
    def __init__(self, config, scale=1.5):
        super(SDFNCT, self).__init__()
        
        assert isinstance(config, Config), 'config must be an instance of class Config'
        
        self.config = config
        x,y = np.meshgrid(np.linspace(0,1,self.config.IMAGE_RESOLUTION),np.linspace(0,1,self.config.IMAGE_RESOLUTION)) 
  
        self.pts = torch.autograd.Variable(2*(torch.from_numpy(np.hstack((x.reshape(-1,1),y.reshape(-1,1)))).cuda().float()-0.5),requires_grad=True)
        
        # SIREN network:
        # in_features = 2 or 3 since input is always a coordinate. 
        # out_features = channels of pixel value, in greyscale = 1, RGB = 3, in our case = num_SDFs
        self.encoder = Siren(2,256,3,config.NUM_SDFS).cuda()

        self.velocity = FourierFeatures(2, config.NUM_SDFS, scale=scale).cuda()
        
    def compute_sdf_t(self, t):
        assert isinstance(t, torch.Tensor) or isinstance(t, float), 't = {} must be a float or a tensor here'.format(t)
        assert t >= -1 and t <= 1, 't = {} is out of range'.format(t)
        
        displacement = self.velocity(self.pts, t)
        init_sdf = self.encoder(self.pts)
        assert init_sdf.shape == displacement.shape
        
        canvas = (init_sdf + displacement)*self.config.SDF_SCALING  
        if not (torch.min(canvas) < -1 and torch.max(canvas) > 1):
            warnings.warn('SDF values are in a narrow range between (-1,1)')
            
        canvas = canvas.view(self.config.IMAGE_RESOLUTION,self.config.IMAGE_RESOLUTION,self.config.NUM_SDFS)
        
        return canvas
            
    def forward(self, t):  # t here is an angle
        
        assert isinstance(t, float), 't = {} must be a float here'.format(t)
        assert t >= -self.config.THETA_MAX and t <= self.config.THETA_MAX, 't = {} is out of range'.format(t)
        
        t = 2*get_phase(self.config,t) - 1  
        
        canvas = self.compute_sdf_t(t)  
        assert len(canvas.shape) == 3, 'Canvas must be a 3D tensor, instead is of shape: {}'.format(canvas.shape)
        
        return canvas
    
    def grad(self, t):
        
        assert isinstance(t, float), 't = {} must be a float here'.format(t)
        assert t >= -self.config.THETA_MAX and t <= self.config.THETA_MAX, 't = {} is out of range'.format(t)
        
        t = torch.autograd.Variable(torch.Tensor([2*get_phase(self.config,t) - 1]).cuda().float(),requires_grad=True)
        
        canvas = self.compute_sdf_t(t)/self.config.SDF_SCALING
        
        # Loss function calculation:
        dc_dxy = gradient(canvas, self.pts)     
        assert len(dc_dxy.shape) == 2, 'Must be a 2D tensor, instead is {}'.format(dc_dxy.shape)

        occupancy = sdf_to_occ(canvas)           
        assert len(occupancy.shape) == 3
        
        do_dxy = gradient(occupancy, self.pts)    
        assert len(do_dxy.shape) == 2, 'Must be a 2D tensor, instead is {}'.format(do_dxy.shape)
        
        dc_dt = gradient(occupancy, t)/(np.prod(canvas.shape)) 
        assert len(dc_dt.shape) == 1, 'Must be a 1D tensor, instead is {}'.format(dc_dt.shape)
        
        eikonal = torch.abs(torch.norm(dc_dxy, dim=1) - 1).mean()
        total_variation_space = torch.norm(do_dxy, dim=1).mean()
        total_variation_time = torch.abs(dc_dt)
        
        return eikonal, total_variation_space, total_variation_time


def get_pretraining_sdfs_spatial_GMM(config, label_image, fbp_movie): # fbp_movie is the recon matrix with each channel as one gantry click
    '''this function returns a movie_object (Size: (x_dim, y_dim, gantry_clicks, num_SDF)) which assigns SDF values to each pixel in the fbp recon.'''
    ''' init is the median intensity for pixels belonged to that componenet.'''
    print("Computing Segmentations...")

    movie = label_image.copy()
    num_components = config.NUM_SDFS
    
    movie_objects = np.zeros((movie.shape[0],movie.shape[1],movie.shape[2],num_components))
    labels = np.arange(0,np.max(movie[...,0])).astype(np.int) # labels are all the labels except the last one, e.g. image's labels = [0,1], the variable labels = [0]

    # make each component as one channel, and turn the label to boolean values.
    # if num_components = 1, then l = 1.
    for i in range(movie.shape[2]):
        for l in labels:
            movie_objects[...,i,l] = (movie[...,i] ==l+1)
           
    print("Computing SDFs...")
    init = np.zeros((1,num_components))
    for j in tqdm(range(num_components)):
        for i in range(movie.shape[2]):
            occupancy = np.round(denoise_tv_chambolle(movie_objects[...,i,j][...,np.newaxis])) # add one more dimension for occupancy, e.g (x_dim, y_dim, 720, num_components) -> (x_dim, y_dim, 720, num_compoenents, 1)
            movie_objects[...,i,j] = denoise_tv_chambolle(occ_to_sdf(occupancy), weight=2)[...,0] #the greater weight, the more denoising
    
    idx = 0
    
    while True:
        break_sign = 0
        init = np.zeros((1,num_components))
        for j in range(0,num_components):
            img = fbp_movie[...,j]
            test = np.where(movie_objects[...,idx,j]>0)#[...,0]  # all the pixels in the first gantry image with SDF > 0
            init[0,j] = np.median(img[test[0], test[1]]) # test[0] is x coordiante and test[1] is y_coordinate
        
        s = [math.isnan(init[0,k]) for k in range(0,init.shape[-1])]
        if sum(s) == 0:
            break
        idx += 1
   
    # movie_objects.shape = [x_dim, y_dim, num_gantry_clicks, num_SDF]
    pretraining_sdfs = movie_objects.copy()

    return pretraining_sdfs, init


def fetch_movie(config, sdf, all_thetas=None):
    frames = np.zeros((config.IMAGE_RESOLUTION,config.IMAGE_RESOLUTION,config.TOTAL_CLICKS,config.NUM_SDFS))
    for i in range(config.NUM_SDFS):
        if all_thetas is None:
            all_thetas = np.linspace(0., config.THETA_MAX, config.TOTAL_CLICKS)
        for j in range(config.TOTAL_CLICKS):
            frames[...,j,i] = sdf_to_occ(sdf(all_thetas[j]))[...,i].detach().cpu().numpy()
    
    
    intensities = config.INTENSITIES.reshape(1,1,1,-1)
    movie = np.sum(frames*intensities, axis=3)

    print('Frame has dimensions as: ', frames.shape)  #[x_dim,y_dim,num_gantry_click,num_SDF]

    return movie       


def save_movie(movie, file_folder):
    assert isinstance(movie, np.ndarray) and len(movie.shape) == 3, 'movie must be a 3D numpy array'
    
    #os.system('rm -r {}/ && mkdir {}'.format(file_folder))
    
    for i in range(movie.shape[2]):
        plt.imsave('{}/{}.png'.format(file_folder,i), movie[...,i], cmap='gray')


# def find_background_channel(image):
    
#     assert isinstance(image, np.ndarray) and len(image.shape) == 3
    
#     total = []
#     for i in range(image.shape[2]):
#         total.append(np.sum(image[...,i]))
        
#     return total.index(max(total))

